{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: paths + load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "## check paths exist  \n",
    "\n",
    "import os \n",
    "gazzetteer_snomed = \"./gazzeteer_snomedct_full.tsv\" # TODO: path to the .tsv of snomed\n",
    "\n",
    "# path to folder with .txt files\n",
    "data_path = \"\" # TODO: path to folder with .txt files\n",
    "\n",
    "print(os.path.exists(gazzetteer_snomed))\n",
    "print(os.path.exists(data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load snomed.tsv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_snomed(gazzetteer_snomed_path):\n",
    "    df_snomed = pd.read_csv(gazzetteer_snomed_path, sep=\"\\t\")\n",
    "    return df_snomed\n",
    "\n",
    "\n",
    "# load data \n",
    "def load_data(data_folder_path):\n",
    "    # Initialize an empty list to store the data\n",
    "    data = []\n",
    "    # Loop through each file in the folder\n",
    "    for file_name in os.listdir(data_folder_path):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(data_folder_path, file_name)\n",
    "            \n",
    "            # Open and read the content of the file\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "            \n",
    "            # Append the file name and content to the list as tuple\n",
    "            data.append({\"id\": file_name, \"text\": content})\n",
    "\n",
    "    # Create a df from the list\n",
    "    df_data = pd.DataFrame(data)\n",
    "    # Display the df\n",
    "    print(df_data.head(2))\n",
    "\n",
    "    return df_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = load_data(data_path)\n",
    "df_snomed =  load_snomed(gazzetteer_snomed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: removal lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removal list for SNOMED: skip following semantic tag, so they will not be annotated\n",
    "\n",
    "removal_list_1 = ['qualifier value', 'dose form', 'basic dose form', 'unit of presentation', 'intended site', 'record artifact',  'attribute',  'SNOMED RT+CTV3', 'navigational concept',  'foundation metadata concept', 'core metadata concept',  'administration method', 'link assertion',  'OWL metadata concept', 'release characteristic', 'namespace concept', 'nan', 'linkage concept', 'special concept',  'supplier',  'context-dependent category']\n",
    "\n",
    "removal_list_2 =  [ 'occupation', 'environment', 'ethnic group', 'geographic location',  'religion/philosophy',  'person' 'racial group']\n",
    "\n",
    "removal_list_3 = ['social concept',  'attribute', 'qualifier value', 'basic dose form', 'unit of presentation', 'intended site',  'attribute',  'SNOMED RT+CTV3', 'navigational concept',  'foundation metadata concept', 'core metadata concept',  'administration method', 'link assertion',  'OWL metadata concept', 'release characteristic', 'namespace concept', 'nan', 'linkage concept', 'special concept',  'supplier',  'context-dependent category']\n",
    "\n",
    "removal_list_4= list(set(removal_list_2 + removal_list_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Normalize the text to ensure consistent handling of accented chars\n",
    "    \"\"\"\n",
    "    return unicodedata.normalize('NFC', text)\n",
    "\n",
    "def dictionary_lookup_snomed(df_snomed, df_data, removal_list=None):\n",
    "    annotations = []\n",
    "\n",
    "    for index, row in df_data.iterrows():\n",
    "        text = normalize_text(row['text'])  # Normalize the text to handle accents properly\n",
    "        text_annotations = []\n",
    "        matched_terms = set()  # Avoid duplicate annotations\n",
    "        occupied_ranges = []\n",
    "\n",
    "        for _, snomed_row in df_snomed.iterrows():\n",
    "            term = snomed_row['term']\n",
    "            code = snomed_row['code']\n",
    "            semantic_tag = snomed_row['semantic_tag']\n",
    "\n",
    "            # Skip terms with semantic tags present in the removal list\n",
    "            if removal_list and semantic_tag in removal_list:\n",
    "                continue\n",
    "\n",
    "            # Normalize the SNOMED term to match the text properly\n",
    "            term = normalize_text(term)\n",
    "\n",
    "            # Match base term and account for hyphenated extensions\n",
    "            pattern = r'\\b' + re.escape(term) + r'([-\\w]*)\\b'\n",
    "            matches = list(re.finditer(pattern, text, flags=re.IGNORECASE))\n",
    "\n",
    "            for match in matches:\n",
    "                start_idx = match.start()\n",
    "                end_idx = match.end()\n",
    "\n",
    "                # Get the full term matched, including additional parts (like \"-brazo\")\n",
    "                full_annotation = text[start_idx:end_idx].strip()\n",
    "                \n",
    "                # Ensure we are capturing the full extent of terms like \"tobillo-brazo\"\n",
    "                actual_start = text.find(full_annotation, start_idx)\n",
    "                actual_end = actual_start + len(full_annotation)\n",
    "\n",
    "                # Validate that the extracted span matches the term in the text\n",
    "                if text[actual_start:actual_end] != full_annotation:\n",
    "                    print(f\"Warning: Span mismatch for term '{full_annotation}' at position {actual_start}:{actual_end}. Adjusting span.\")\n",
    "                    continue  # Skip mismatched annotations\n",
    "\n",
    "                # Check for overlap with existing annotations\n",
    "                if not is_overlap(actual_start, actual_end, occupied_ranges) and full_annotation not in matched_terms:\n",
    "                    # Normalize semantic_tag by replacing spaces with underscores\n",
    "                    semantic_tag_normalized = semantic_tag.replace(' ', '_')\n",
    "\n",
    "                    # Track the annotation and prevent overlaps\n",
    "                    text_annotations.append({\n",
    "                        'start': actual_start,\n",
    "                        'end': actual_end,\n",
    "                        'term': full_annotation,\n",
    "                        'code': code,\n",
    "                        'label': f'SNOMED_{semantic_tag_normalized}',  # Use normalized semantic_tag\n",
    "                        'original': term  # The exact term matched, like \"tobillo\"\n",
    "                    })\n",
    "                    occupied_ranges.append((actual_start, actual_end))\n",
    "                    matched_terms.add(full_annotation)\n",
    "\n",
    "        annotations.append({\n",
    "            'index': index,\n",
    "            'id': row['id'].replace('.txt', ''),\n",
    "            'text': text,\n",
    "            'annotations': text_annotations\n",
    "        })\n",
    "\n",
    "    return annotations\n",
    "\n",
    "def is_overlap(start, end, occupied_ranges):\n",
    "    \"\"\" Check if the annotation overlaps with existing ranges -- Important \"\"\"\n",
    "    return any((start < range_end and end > range_start) for range_start, range_end in occupied_ranges)\n",
    "\n",
    "def create_brat_annotations(annotations, output_folder):\n",
    "    \"\"\"\n",
    "    Creates .txt and .ann files in BRAT format for each annotation\n",
    "    Adds a check for proper annotation spans\n",
    "    \"\"\"\n",
    "    # Ensure the output folder exists\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for annotation in annotations:\n",
    "        text_id = annotation['id'].replace('.txt', '')  # Ensure 'id' doesn't contain '.txt'\n",
    "        text = annotation['text']\n",
    "        text_annotations = annotation['annotations']\n",
    "\n",
    "        # Define file paths for both .txt and .ann files\n",
    "        txt_file_path = os.path.join(output_folder, f'{text_id}.txt')\n",
    "        ann_file_path = os.path.join(output_folder, f'{text_id}.ann')\n",
    "\n",
    "        # Save the original text in a .txt file\n",
    "        with open(txt_file_path, 'w', encoding='utf-8') as txt_file:\n",
    "            txt_file.write(text)\n",
    "\n",
    "        # Save the annotations in a .ann file\n",
    "        with open(ann_file_path, 'w', encoding='utf-8') as ann_file:\n",
    "            for i, ann in enumerate(text_annotations):\n",
    "                start = ann['start']\n",
    "                end = ann['end']\n",
    "                term = ann['term']\n",
    "\n",
    "                # Check if the annotated span matches the actual text\n",
    "                if text[start:end] != term:\n",
    "                    print(f\"Warning: Span mismatch in file {text_id}. Expected '{term}', found '{text[start:end]}' at {start}:{end}\")\n",
    "\n",
    "                # Write BRAT-style annotation with the correct label\n",
    "                ann_file.write(f\"T{i+1}\\t{ann['label']} {start} {end}\\t{term}\\n\")\n",
    "\n",
    "                # Annotator notes: write the corresponding SNOMED code and original annotation\n",
    "                ann_file.write(f\"#{i+1}\\tAnnotatorNotes T{i+1}\\tSNOMED annotation: {ann['original']}\\tSNOMED_CODE: {ann['code']}\\n\")\n",
    "\n",
    "\n",
    "# Fix encoding/span issues\n",
    "def run_annotation_process(df_snomed, df_data, removal_lists, output_base_folder):\n",
    "    \"\"\"\n",
    "    Runs the annotation process for different removal lists and creates output files\n",
    "    \"\"\"\n",
    "    for list_number, removal_list in removal_lists.items():\n",
    "        # Define output folder based on the removal list number\n",
    "        output_folder = os.path.join(output_base_folder, f'gazzetteer_snomed_removal_list_{list_number}')\n",
    "\n",
    "        # Run the dictionary lookup with the current removal list\n",
    "        annotations = dictionary_lookup_snomed(df_snomed, df_data, removal_list)\n",
    "\n",
    "        # Create the BRAT annotations for this run\n",
    "        create_brat_annotations(annotations, output_folder=output_folder)\n",
    "\n",
    "# Dictionary of removal lists to iterate \n",
    "\n",
    "removal_lists = {\n",
    "    1: removal_list_1 , \n",
    "    2: removal_list_2,\n",
    "    3: removal_list_3,\n",
    "    4: removal_list_4\n",
    "}\n",
    "\n",
    "output_base_folder = \"\" # output path\n",
    "run_annotation_process(df_snomed, df_data, removal_lists, output_base_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
